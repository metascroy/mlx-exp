# Python structure
* ops_schema.py: schema for ops that interpreter runs
* generate_from_schema.py: generates src/ops.hpp for C++ op stubs and generated_ops_mixin.py for python op stubs.
* _program_builder.py: builder/registry for converted an exported program to a serialized json the C++ interpreter understands
* torch_registry.py: registration of torch ops.  This tells the program builder how to convert a torch op to an op in generated_ops_mixin.py
* program_builder.py: public facing version of _program_builder.py.  Import from this to get a ProgramBuilder with registered ops.

# C++ structure
* src/program.hpp: program structure that the interpreter runs
* src/interpreter.hpp: interpreter that loops over ops in program
* src/program_json_loader.hpp: loader to convert program.json to program object in src/program.hpp
* src/ops.hpp: op nodes.  This file is auto-generated by the python script generate_from_schema.py based on ops_schema.py

# End to end test with Llama: from torch.export to execution by MLX interpreter

To run a model, we need 3 files:
 * A json file indicating the program structure
 * A safetensor file with constant data like weight parameters
 * A file with token ids

 To generate these files for llama1B run `export_llama.py`.
 This pulls a model from huggingface, exports it to an ExportedProgram, and builds a program for the interpreter.  Running it will create 3 files:
 * prog.json (program structure like inputs, outputs, op list, etc)
 * consts.safetensors (constant weight data)
 * prompt_ids.txt (a list of prompt ids for testing; the specific prompt is encoded in export_llama.py)

Once this runs, you can run the program in the interpreter by running `sh build_and_run.sh`.
You might have to adjust `src/main.cpp` to point to the right locations for the above files:

```
try {
    const std::string prog_json   = env_or("PROG_JSON",   "/Users/scroy/repos/mlx-exp/prog.json");
    const std::string consts_path = env_or("CONSTS_ST",   "/Users/scroy/repos/mlx-exp/consts.safetensors");
    const std::string prompt_fp   = env_or("PROMPT_IDS",  "/Users/scroy/repos/mlx-exp/prompt_ids.txt");
    const int max_new_tokens      = env_or_int("MAX_NEW_TOKENS", 128);
    const int print_batch         = std::max(1, env_or_int("PRINT_BATCH", 1));
    const std::string output_ids  = env_or("OUTPUT_IDS",  "/Users/scroy/repos/mlx-exp/output_ids.txt");
```

After this runs, the output will be present in output_ids.txt.  This can be decoded into text by running `python decode_llama_output_ids.py`.
 
